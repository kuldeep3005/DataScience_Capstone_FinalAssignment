ptm <- proc.time()
saveRDS(train_tokens,"train_tokens.Rdata")
train_tokens <- readRDS("train_tokens.Rdata")
proc.time()-ptm
####################################
ptm <- proc.time()
# register parallel backend
N_WORKERS <- 4
registerDoParallel(N_WORKERS)
# define number of splits
N_SPLITS <- 4
jobs <- train_tokens %>%
split_into(N_SPLITS) %>%
lapply(itoken)
rm(train_tokens)
# UNIGRAM text2vec object
vocab <- create_vocabulary(jobs, ngram = c(1L, 1L))
pruned_vocab <- prune_vocabulary(vocab,term_count_min = 10)
rm(vocab)
freqr <- pruned_vocab[1]
rm(pruned_vocab)
freqr <- data.frame(freqr)
freqr <- freqr[, 1:2]
freqr <- freqr %>% arrange(desc(vocab.terms_counts)) %>%
rename(word = vocab.terms, frequency = vocab.terms_counts)
saveRDS(freqr,"unigrams.Rdata")
rm(freqr)
gc()
# BIGRAM text2vec object
vocab2 <- create_vocabulary(jobs, ngram = c(2L, 2L))
pruned_vocab2 <- prune_vocabulary(vocab2,term_count_min = 4)
rm(vocab2)
freqr2 <- pruned_vocab2[1]
rm(pruned_vocab2)
freqr2 <- data.frame(freqr2)
freqr2 <- freqr2[, 1:2]
freqr2 <- freqr2 %>% arrange(desc(vocab.terms_counts)) %>%
rename(word = vocab.terms, frequency = vocab.terms_counts)
spl <- strsplit(freqr2$word,"_")
w1 <- sapply(spl,"[[",1)
w2 <- sapply(spl,"[[",2)
bi_df <- data.frame(w1,w2,freqr2$frequency)
rm(spl,w1,w2,freqr2)
saveRDS(bi_df,"bigrams.Rdata")
rm(bi_df)
gc()
# trigram text2vec object
vocab3 <- create_vocabulary(jobs, ngram = c(3L, 3L))
pruned_vocab3 <- prune_vocabulary(vocab3,term_count_min = 2)
rm(vocab3)
freqr3 <- pruned_vocab3[1]
rm(pruned_vocab3)
freqr3 <- data.frame(freqr3)
freqr3 <- freqr3[, 1:2]
freqr3 <- freqr3 %>% arrange(desc(vocab.terms_counts)) %>%
rename(word = vocab.terms, frequency = vocab.terms_counts)
spl <- strsplit(freqr3$word,"_")
w1 <- sapply(spl,"[[",1)
w2 <- sapply(spl,"[[",2)
w3 <- sapply(spl,"[[",3)
tri_df <- data.frame(w1,w2,w3,freqr3$frequency)
rm(spl,w1,w2,w3,freqr3)
saveRDS(tri_df,"trigrams.Rdata")
rm(tri_df)
gc()
# quadgram text2vec object
vocab4 <- create_vocabulary(jobs, ngram = c(4L, 4L))
pruned_vocab4 <- prune_vocabulary(vocab4,term_count_min = 2)
rm(vocab4)
freqr4 <- pruned_vocab4[1]
rm(pruned_vocab4)
freqr4 <- data.frame(freqr4)
freqr4 <- freqr4[, 1:2]
freqr4 <- freqr4 %>% arrange(desc(vocab.terms_counts)) %>%
rename(word = vocab.terms, frequency = vocab.terms_counts)
spl <- strsplit(freqr4$word,"_")
w1 <- sapply(spl,"[[",1)
w2 <- sapply(spl,"[[",2)
w3 <- sapply(spl,"[[",3)
w4 <- sapply(spl,"[[",4)
quad_df <- data.frame(w1,w2,w3,w4,freqr4$frequency)
rm(spl,w1,w2,w3,w4,freqr4)
saveRDS(quad_df,"quadgrams.Rdata")
rm(quad_df)
gc()
# pentagram text2vec object
vocab5 <- create_vocabulary(jobs, ngram = c(5L, 5L))
pruned_vocab5 <- prune_vocabulary(vocab5,term_count_min = 2)
rm(vocab5)
freqr5 <- pruned_vocab5[1]
rm(pruned_vocab5)
freqr5 <- data.frame(freqr5)
freqr5 <- freqr5[, 1:2]
freqr5 <- freqr5 %>% arrange(desc(vocab.terms_counts)) %>%
rename(word = vocab.terms, frequency = vocab.terms_counts)
spl <- strsplit(freqr5$word,"_")
w1 <- sapply(spl,"[[",1)
w2 <- sapply(spl,"[[",2)
w3 <- sapply(spl,"[[",3)
w4 <- sapply(spl,"[[",4)
w5 <- sapply(spl,"[[",5)
penta_df <- data.frame(w1,w2,w3,w4,w5,freqr5$frequency)
rm(spl,w1,w2,w3,w4,w5,freqr5)
saveRDS(penta_df,"pentagrams.Rdata")
rm(penta_df)
gc()
# ectagram text2vec object
vocab6 <- create_vocabulary(jobs, ngram = c(6L, 6L))
pruned_vocab6 <- prune_vocabulary(vocab6,term_count_min = 2)
rm(vocab6)
freqr6 <- pruned_vocab6[1]
rm(pruned_vocab6)
freqr6 <- data.frame(freqr6)
freqr6 <- freqr6[, 1:2]
freqr6 <- freqr6 %>% arrange(desc(vocab.terms_counts)) %>%
rename(word = vocab.terms, frequency = vocab.terms_counts)
spl <- strsplit(freqr6$word,"_")
w1 <- sapply(spl,"[[",1)
w2 <- sapply(spl,"[[",2)
w3 <- sapply(spl,"[[",3)
w4 <- sapply(spl,"[[",4)
w5 <- sapply(spl,"[[",5)
w6 <- sapply(spl,"[[",6)
ecta_df <- data.frame(w1,w2,w3,w4,w5,w6,freqr6$frequency)
rm(spl,w1,w2,w3,w4,w5,w6,freqr6)
saveRDS(ecta_df,"ectagrams.Rdata")
rm(ecta_df)
gc()
proc.time() - ptm
######################################
ptm <- proc.time()
uni_df <- readRDS("unigrams.Rdata")
bi_df <- readRDS("bigrams.Rdata")
tri_df <- readRDS("trigrams.Rdata")
quad_df <- readRDS("quadgrams.Rdata")
penta_df <- readRDS("pentagrams.Rdata")
ecta_df <- readRDS("ectagrams.Rdata")
proc.time()-ptm
####################################
help("Deprecated")
# Coursera DS Capstone Project
# Preparing data for downstream analysis and models
# 2019-09-05
# Libraries and options ####
ptm <- proc.time()
library(tm)
library(ggplot2)
library(dplyr)
library(SnowballC)
library(stringi)
library(doParallel)
library(text2vec)
setwd("C:/Users/kuldeep.singh.meena/Downloads/R-Data/Capstone/Final_assignment")
source("./used_functions.R")
proc.time() - ptm
################################
ptm <- proc.time()
twitter.data <- "./Data/final/en_US/en_US.twitter.txt"
blog.data <- "./Data/final/en_US/en_US.blogs.txt"
news.data <- "./Data/final/en_US/en_US.news.txt"
twitter.file <- file(twitter.data,"rb")
twitter <- readLines(twitter.file, skipNul = TRUE, encoding = "UTF-8")
close(twitter.file)
blog.file <- file(blog.data,"rb")
blog <- readLines(blog.file, skipNul = TRUE, encoding = "UTF-8")
close(blog.file)
news.file <- file(news.data,"rb")
news <- readLines(news.file, skipNul = TRUE, encoding = "UTF-8")
close(news.file)
profanities = readLines('bad-words.txt')
proc.time()-ptm
###################################
ptm <- proc.time()
all <- c(twitter,blog,news)
rm(twitter,twitter.data,twitter.file,blog,blog.data,blog.file,news,news.data,news.file)
proc.time() - ptm
###################################
ptm <- proc.time()
set.seed(1220)
all2 <- sample(all,100000)
rm(all)
train_tokens <- all2 %>%
graphToSpace %>%
tolower %>%
removeConfounders %>%
removePunctuation %>%
removeNumbers %>%
removeWords(profanities) %>%
removeWords(letters[!letters %in% c("a","i")]) %>%
stripWhitespace
rm(all2)
proc.time() - ptm
####################################
ptm <- proc.time()
saveRDS(train_tokens,"train_tokens.Rdata")
train_tokens <- readRDS("train_tokens.Rdata")
proc.time()-ptm
####################################
ptm <- proc.time()
# register parallel backend
N_WORKERS <- 4
registerDoParallel(N_WORKERS)
# define number of splits
N_SPLITS <- 4
jobs <- train_tokens %>%
split_into(N_SPLITS) %>%
lapply(itoken)
rm(train_tokens)
# UNIGRAM text2vec object
vocab <- create_vocabulary(jobs, ngram = c(1L, 1L))
pruned_vocab <- prune_vocabulary(vocab,term_count_min = 10)
rm(vocab)
freqr <- pruned_vocab[1]
rm(pruned_vocab)
freqr <- data.frame(freqr)
freqr <- freqr[, 1:2]
freqr <- freqr %>% arrange(desc(vocab.terms_counts)) %>%
rename(word = vocab.terms, frequency = vocab.terms_counts)
saveRDS(freqr,"unigrams.Rdata")
rm(freqr)
gc()
# BIGRAM text2vec object
vocab2 <- create_vocabulary(jobs, ngram = c(2L, 2L))
pruned_vocab2 <- prune_vocabulary(vocab2,term_count_min = 4)
rm(vocab2)
freqr2 <- pruned_vocab2[1]
rm(pruned_vocab2)
freqr2 <- data.frame(freqr2)
freqr2 <- freqr2[, 1:2]
freqr2 <- freqr2 %>% arrange(desc(vocab.terms_counts)) %>%
rename(word = vocab.terms, frequency = vocab.terms_counts)
spl <- strsplit(freqr2$word,"_")
w1 <- sapply(spl,"[[",1)
w2 <- sapply(spl,"[[",2)
bi_df <- data.frame(w1,w2,freqr2$frequency)
rm(spl,w1,w2,freqr2)
saveRDS(bi_df,"bigrams.Rdata")
rm(bi_df)
gc()
# trigram text2vec object
vocab3 <- create_vocabulary(jobs, ngram = c(3L, 3L))
pruned_vocab3 <- prune_vocabulary(vocab3,term_count_min = 2)
rm(vocab3)
freqr3 <- pruned_vocab3[1]
rm(pruned_vocab3)
freqr3 <- data.frame(freqr3)
freqr3 <- freqr3[, 1:2]
freqr3 <- freqr3 %>% arrange(desc(vocab.terms_counts)) %>%
rename(word = vocab.terms, frequency = vocab.terms_counts)
spl <- strsplit(freqr3$word,"_")
w1 <- sapply(spl,"[[",1)
w2 <- sapply(spl,"[[",2)
w3 <- sapply(spl,"[[",3)
tri_df <- data.frame(w1,w2,w3,freqr3$frequency)
rm(spl,w1,w2,w3,freqr3)
saveRDS(tri_df,"trigrams.Rdata")
rm(tri_df)
gc()
# quadgram text2vec object
vocab4 <- create_vocabulary(jobs, ngram = c(4L, 4L))
pruned_vocab4 <- prune_vocabulary(vocab4,term_count_min = 2)
rm(vocab4)
freqr4 <- pruned_vocab4[1]
rm(pruned_vocab4)
freqr4 <- data.frame(freqr4)
freqr4 <- freqr4[, 1:2]
freqr4 <- freqr4 %>% arrange(desc(vocab.terms_counts)) %>%
rename(word = vocab.terms, frequency = vocab.terms_counts)
spl <- strsplit(freqr4$word,"_")
w1 <- sapply(spl,"[[",1)
w2 <- sapply(spl,"[[",2)
w3 <- sapply(spl,"[[",3)
w4 <- sapply(spl,"[[",4)
quad_df <- data.frame(w1,w2,w3,w4,freqr4$frequency)
rm(spl,w1,w2,w3,w4,freqr4)
saveRDS(quad_df,"quadgrams.Rdata")
rm(quad_df)
gc()
# pentagram text2vec object
vocab5 <- create_vocabulary(jobs, ngram = c(5L, 5L))
pruned_vocab5 <- prune_vocabulary(vocab5,term_count_min = 2)
rm(vocab5)
freqr5 <- pruned_vocab5[1]
rm(pruned_vocab5)
freqr5 <- data.frame(freqr5)
freqr5 <- freqr5[, 1:2]
freqr5 <- freqr5 %>% arrange(desc(vocab.terms_counts)) %>%
rename(word = vocab.terms, frequency = vocab.terms_counts)
spl <- strsplit(freqr5$word,"_")
w1 <- sapply(spl,"[[",1)
w2 <- sapply(spl,"[[",2)
w3 <- sapply(spl,"[[",3)
w4 <- sapply(spl,"[[",4)
w5 <- sapply(spl,"[[",5)
penta_df <- data.frame(w1,w2,w3,w4,w5,freqr5$frequency)
rm(spl,w1,w2,w3,w4,w5,freqr5)
saveRDS(penta_df,"pentagrams.Rdata")
rm(penta_df)
gc()
# ectagram text2vec object
vocab6 <- create_vocabulary(jobs, ngram = c(6L, 6L))
pruned_vocab6 <- prune_vocabulary(vocab6,term_count_min = 2)
rm(vocab6)
freqr6 <- pruned_vocab6[1]
rm(pruned_vocab6)
freqr6 <- data.frame(freqr6)
freqr6 <- freqr6[, 1:2]
freqr6 <- freqr6 %>% arrange(desc(vocab.terms_counts)) %>%
rename(word = vocab.terms, frequency = vocab.terms_counts)
spl <- strsplit(freqr6$word,"_")
w1 <- sapply(spl,"[[",1)
w2 <- sapply(spl,"[[",2)
w3 <- sapply(spl,"[[",3)
w4 <- sapply(spl,"[[",4)
w5 <- sapply(spl,"[[",5)
w6 <- sapply(spl,"[[",6)
ecta_df <- data.frame(w1,w2,w3,w4,w5,w6,freqr6$frequency)
rm(spl,w1,w2,w3,w4,w5,w6,freqr6)
saveRDS(ecta_df,"ectagrams.Rdata")
rm(ecta_df)
gc()
proc.time() - ptm
######################################
ptm <- proc.time()
uni_df <- readRDS("unigrams.Rdata")
bi_df <- readRDS("bigrams.Rdata")
tri_df <- readRDS("trigrams.Rdata")
quad_df <- readRDS("quadgrams.Rdata")
penta_df <- readRDS("pentagrams.Rdata")
ecta_df <- readRDS("ectagrams.Rdata")
proc.time()-ptm
####################################
shiny::runApp()
getwd()
list.files(paths,
pattern = "^00LOCK*|*\\.rds$|*\\.RDS$",
full.names = TRUE)
runApp()
unlink('C:/Users/kuldeep.singh.meena/Downloads/R-Data/Course9/week2/Assignment/First_Leaflet_Map_cache', recursive = TRUE)
library(dplyr)
library(knitr)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
DS-Capstone-Final-Project.Rmd
========================================================
title: Data Science Capstone Project
author: Kuldeep Singh Meena
date: September 7th, 2019
autosize: true
Project Overview
========================================================
The purpose of this project is to create a Word Prediction application with R shiny package that predicts next words using NLP(Natural Language Processing) model.
- Given a word.phrase as input, the application tries to predict next words, very similiar to the way most      smartphones predict next words using "Swiftkey"
- Prediction Model has been trained using subsets from three datasets(blogs, twitter and news)
- It populates next predicted word as an output.
Using Word Prediction App
========================================================
<div style="align:top"><img src="./www/App_UserGuide.png" alt="User Guide" /></div>
Prediction Model Overview - Data Gathering and Cleansing
========================================================
- Data have been merged from 3 datasets(Blogs, Twitter and News) into one data file.
- Data were cleansed by converting them to lowercase, stripping white space and removing punctuation & numbers.
- N-grams like Bigram, Trigram and Quadgram were created using cleansed dataset.
- Extracted term-count tables from the n-grams.
- Sorted in descending order based on frequency.
- Saved n-gram objects.
Word Prediction Algorithm & Summary
========================================================
- Algorithm used here, checks highest order of n-gram(n=4)
- If n=4 is not found, then it checks the next lower-order model (n=3)
- If n=3 is not found, then app continues to check (n=2)
- If n=2 is not found, then application returns "No Match Found"
- Word Prediction App is hosted on Shinyapps.io(link: )
- Code and presentation are available in Github repo(link: https://github.com/kuldeep3005/Capstone_Final_Assignment)
runApp()
runApp()
runApp('Capstone-Word-Prediction')
runApp('Capstone-Word-Prediction')
runApp('Capstone-Word-Prediction')
runApp()
runApp()
runApp()
runApp('Capstone-Word-Prediction')
runApp('Capstone-Word-Prediction')
runApp('Capstone-Word-Prediction')
runApp('Capstone-Word-Prediction')
runApp('Capstone-Word-Prediction')
runApp('Capstone-Word-Prediction')
runApp('Capstone-Word-Prediction')
runApp('Capstone-Word-Prediction')
runApp('Capstone-Word-Prediction')
suppressWarnings(library(shiny))
suppressWarnings(library(markdown))
# Next Word Prediction Application UI
shinyUI(navbarPage("Capstone: Final Assignment- Prediction Model",
tabPanel("Predict Next Word",
HTML("<strong>Author: Kuldeep Singh Meena</strong>"),
br(),
HTML("<strong>Date: 05/09/2019</strong>"),
br(),
HTML('<img src="Coursera.png", height="40px"
style="float:left"/>','<p style="color:black"></p>'),
#title for left side of page
titlePanel("Text Prediction: User Interface"),
# Sidebar for user to enter part of a sentence
sidebarLayout(
sidebarPanel(
helpText("This is prototype model of Neuro Linguistic Programming techniques to predict next word"),
textInput("inputString", "Please Enter a word, Text or a sentence",value = ""),
br(),
br(),
img(src = "Swiftkey.png"),
br(),
br(),
br()
),
# Main panel to display the results of the word prediction
mainPanel(
h2("Next Word Prediction"),
verbatimTextOutput("prediction"),
strong("Here is what the user has entered:"),
tags$style(type='text/css', '#text1 {background-color: rgba(102,255,102,0.50); color: black;}'),
textOutput('text1'),
br(),
strong("Here is how the next word was Predicted:"),
tags$style(type='text/css', '#text2 {background-color: rgba(130,255,180,0.30); color: black;}'),
textOutput('text2'),
br(),
br(),
br()
)
)
),
tabPanel("About",
mainPanel(
img(src = "Coursera.png"),
br(),
img(src = "Swiftkey.png"),
img(src = "R_Studio.jpg",  height = 70, width = 210),
br(),
includeMarkdown("./about.md")
)
)
)
)
suppressPackageStartupMessages(c(
library(shiny),
library(tm),
library(stringr)))
# Loading Bigram, Trigram and Quadgram Data frame files
quadgram <- readRDS("quadgram.RData");
trigram  <- readRDS("trigram.RData");
bigram   <- readRDS("bigram.RData");
mesg <<- ""
# Cleaning user Input before start Prediction of next word
Predict <- function(x) {
xclean <- removeNumbers(removePunctuation(tolower(x)))
xs <- strsplit(xclean, " ")[[1]]
# Predict the next term of the user input sentence
# 1. For prediction of the next word, Quadgram is used first
# 2. If no Quadgram is found, then the Trigram is used
# 3. If no Trigram is found, then the Bigram is used
# 4. If no Bigram is found, then the word with highest frequency is returned
if (length(xs)>= 3) {
xs <- tail(xs,3)
if (identical(character(0),head(quadgram[quadgram$unigram == xs[1] & quadgram$bigram == xs[2] & quadgram$trigram == xs[3], 4],1))){
Predict(paste(xs[2],xs[3],sep=" "))
}
else {mesg <<- "Next word is predicted using 4-gram."; head(quadgram[quadgram$unigram == xs[1] & quadgram$bigram == xs[2] & quadgram$trigram == xs[3], 4],1)}
}
else if (length(xs) == 2){
xs <- tail(xs,2)
if (identical(character(0),head(trigram[trigram$unigram == xs[1] & trigram$bigram == xs[2], 3],1))) {
Predict(xs[2])
}
else {mesg<<- "Next word is predicted using 3-gram."; head(trigram[trigram$unigram == xs[1] & trigram$bigram == xs[2], 3],1)}
}
else if (length(xs) == 1){
xs <- tail(xs,1)
if (identical(character(0),head(bigram[bigram$unigram == xs[1], 2],1))) {mesg<<-"No match found. Most common word 'the' is returned."; head("the",1)}
else {mesg <<- "Next word is predicted using 2-gram."; head(bigram[bigram$unigram == xs[1],2],1)}
}
}
shinyServer(function(input, output) {
output$prediction <- renderPrint({
result <- Predict(input$inputString)
output$text2 <- renderText({mesg})
result
});
output$text1 <- renderText({
input$inputString});
}
)
suppressPackageStartupMessages(c(
library(shiny),
library(tm),
library(stringr),
library(markdown)))
